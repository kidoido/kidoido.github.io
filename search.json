[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Igor Domaradzki",
    "section": "",
    "text": "Cześć! Witam na mojej stronie, na której znajdziecie moje portforlio Data Science.\nJestem studentem SGH na kierunku magisterskim Analiza danych - big data.\nWcześniej ukończyłem tam licencjat na kierunku Metody ilościowe w ekonomii i systemy informacyjne, gdzie napisałem pracę licencjacką “Zastosowanie teorii gier do analizy wybranych zagadnień w sporcie”\nMoje CV\nPoniżej kilka projektów, które stworzyłem.\n\n\n\nTo ja!\n\n\nStrona stworzona z użyciem RPosit Quarto i opublikowana przez Github Pages.\n\n\n\n\n\n\n\n\n\n\n\n\n\nR Wrapped\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTime Series forecasting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel LLM\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlgorytm genetyczny\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFunction Factory\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCałki podwójne\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR6Class\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nForecasting stocks with ML\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nARMA election forecasting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n:::\n\nNo matching items\n\n:::\n:::"
  },
  {
    "objectID": "Projekty/Integrate3d/index.html",
    "href": "Projekty/Integrate3d/index.html",
    "title": "6. Całki podwójne",
    "section": "",
    "text": "Projekt wykonany w ramach zajęć Podstawy programowania w języku R."
  },
  {
    "objectID": "Projekty/Integrate3d/index.html#opis-projektu",
    "href": "Projekty/Integrate3d/index.html#opis-projektu",
    "title": "6. Całki podwójne",
    "section": "Opis projektu",
    "text": "Opis projektu\nZadanie w tym projekcie jest raczej proste i dotyczy obliczania całek z funkcji f:R^2 -&gt; R na przedziałach.\nW tym celu chcemy zdefiniować funkcję, która przyjmuje dwa główne argumenty: pierwszy reprezentujący matematyczną funkcję, która będzie całkowana oraz drugi, który reprezentuje przedział całkowania. Funkcja może przyjmować dodatkowe argumenty w zależności od wykorzystanego algorytmu (przykład podany poniżej). Funkcja powinna zwracać wartość obliczonej całki.\nZ technicznego punktu widzenia, chcemy zdefiowanować funkcję integrate3d(), która przyjmuje dwa argumenty:\n\nf funkcja, która jest całkowana, np., f(x,y) {x ^ 2 + y ^ 2};\nover lista zawierająca dwie pozycje, reprezentująca przedział całkowania, np., list(x = c(0,1), y = c(0, 1))."
  },
  {
    "objectID": "Projekty/Integrate3d/index.html#moje-rozwiązanie",
    "href": "Projekty/Integrate3d/index.html#moje-rozwiązanie",
    "title": "6. Całki podwójne",
    "section": "Moje rozwiązanie",
    "text": "Moje rozwiązanie\nCałkę obliczam metodą prostokątów. Z racji, że w zadaniu liczymy całki podówjne można ją nazwać metodą równoległoboków.\n\nintegrate3d &lt;- function(f, over, n = 120) {\n  # Dzielimy przestrzeń, na której całkujemy na n^2 części\n  axis_x &lt;- seq(over$x[1], over$x[2], length.out = n)\n  axis_y &lt;- seq(over$y[1], over$y[2], length.out = n)\n  \n  #Liczymy wartość całki w poszczególnych punktach i dodajemy objętości kolejnych prostopadłościanów do siebie\n  integral_sum &lt;- 0\n  for (xx in 1:(n-1)){\n    smaller_x &lt;- axis_x[xx]\n    bigger_x &lt;- axis_x[xx+1]\n    avg_x &lt;- mean(smaller_x,bigger_x)\n    for (yy in 1:(n-1)) {\n      smaller_y &lt;- axis_y[yy]\n      bigger_y &lt;- axis_y[yy+1]\n      avg_y &lt;- mean(smaller_y,bigger_y)\n      z &lt;- f(avg_x, avg_y)\n      parrallelogram_volume &lt;- (bigger_y - smaller_y)*(bigger_x - smaller_x)*z\n      integral_sum &lt;- integral_sum + parrallelogram_volume\n    }\n  }\n  return(integral_sum)\n} \n\n\n### Example 1\nintegrate3d(\n  f = function(x, y) {cos(x) * y},\n  over = list(x = c(0, pi / 2), y = c(0, 1)))\n\n[1] 0.4990634\n\n\n\n### Example 2\nintegrate3d(\n  f = function(x, y) { (cos(x) + 2) * (sin(y) + 1)},\n  over = list(x = c(0, pi), y = c(0, pi)),\n  n = 10^2)\n\n[1] 32.46768\n\n\n\n### Example 3\nintegrate3d(\n  f = function(x, y) {30 + 23*x + (13/4)*y + (28.12)*x*y - 144 * sqrt(x^2 + y^2)},\n  over = list(x = c(0, (5 * pi) / 2), y = c(0, 5)))\n\n[1] -12351.79"
  },
  {
    "objectID": "Projekty/LLM/index.html",
    "href": "Projekty/LLM/index.html",
    "title": "3. Model LLM",
    "section": "",
    "text": "Projekt wykonany w Pythonie w ramach zajęć Big Data"
  },
  {
    "objectID": "Projekty/ARMA_elections/index.html",
    "href": "Projekty/ARMA_elections/index.html",
    "title": "9. ARMA election forecasting",
    "section": "",
    "text": "Projekt wykonany w Pythonie na zajęcia Uczenie maszynowe w Pythonie.\nZadanie zaliczeniowe polega na prognozie wyników pierwszej tury wyborów prezydenckich.\nNiemały kłopot sprawiło mi wybranie odpowiedniej metody do rozwiązania tego problemu. Rozważałem użycie MLPRegressor, Random Forest czy regresji liniowej. Problemem jednak była trudność w zdobyciu dużej ilości danych. Po prostu nie mieliśmy w III RP wystarczająco dużej liczby wyborów. Chciałem, aby moim zbiorem danych było zestawienie sondaży, a zmienną objaśnianą – wynik w wyborach. Pomyślałem, że można by znaleźć jakiś zbiór danych w internecie o takiej strukturze. Jednak ku mojemu zaskoczeniu, żadnego takiego nie znalazłem.\nZdecydowałem się więc zastosować podejście bardziej ekonometryczne niż deep learningowe. W poniższym rozwiązaniu zastosowałem model ARMA, który miałem przyjemność poznać na zajęciach “Applied Macroeconometrics” podczas mojej półrocznej wymiany studenckiej na szwajcarskim Université de Neuchâtel."
  },
  {
    "objectID": "Projekty/ARMA_elections/index.html#plan-mojego-projektu",
    "href": "Projekty/ARMA_elections/index.html#plan-mojego-projektu",
    "title": "9. ARMA election forecasting",
    "section": "Plan mojego projektu:",
    "text": "Plan mojego projektu:\n\nPobranie danych sondażowych dotyczących wyborów\nModyfikacja danych\nDopasowanie modelu\nObliczenie predykcji\nNormalizacja wyników\n\n\nimport requests\nimport pandas as pd\nfrom bs4 import BeautifulSoup\nimport numpy as np\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom sklearn.metrics import mean_absolute_error\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\nPobranie danych z Internetu\n\n# Na poniższej stronie znajduje się tabela ze wszystkimi sondażami\nurl = 'https://ewybory.eu/wybory-prezydenckie-2025-polska/sondaze-prezydenckie-1-tura/'\n\n# Pobranie zawartości strony\nresponse = requests.get(url)\nsoup = BeautifulSoup(response.text, 'html.parser')\ntables = soup.find_all('table')\n\n# Zapis do df\ntry:\n    df = pd.read_html(str(tables[0]))[0]\n    print(df.head())\nexcept Exception as e:\n    print(\"Błąd.\", e)\n\n                        Unnamed: 0  Trzaskowski  Nawrocki  Mentzen  Hołownia  \\\n0  Wybory (I tura) N=1960378418.05        31.36     29.54    14.81      4.99   \n1                  OGB N=100016.05        32.90     28.70    14.20      4.70   \n2          Opinia24 N=100214-15.05        29.00     25.00    12.00      7.00   \n3             IBRiS N=100013-15.05        31.10     25.30    10.80      6.20   \n4    United Surveys N=100013-15.05        31.50     26.60    11.70      7.50   \n\n   Zandberg  Biejat Braun Stanowski Senyszyn Jakubiak Bartoszewicz Maciak  \\\n0      4.86    4.23  6.34      1.24     1.09     0.77         0.49   0.19   \n1      4.50    4.30   5.9       1.4      1.0      1.7          0.4    0.3   \n2      7.00    6.00     4         2        2        2            1      0   \n3      4.60    5.70   3.5       1.5      1.9      1.4          0.4    0.2   \n4      4.40    6.70   4.1       1.8      2.4      2.0            —      —   \n\n   Woch Pozostali  \n0  0.09         —  \n1   0.1         —  \n2     0         —  \n3   0.2         —  \n4     —       1.3  \n\n\n\ndf = df.drop(columns = \"Unnamed: 0\")\ndf.head(10)\n\n\n\n\n\n\n\n\nTrzaskowski\nNawrocki\nMentzen\nHołownia\nZandberg\nBiejat\nBraun\nStanowski\nSenyszyn\nJakubiak\nBartoszewicz\nMaciak\nWoch\nPozostali\n\n\n\n\n0\n31.36\n29.54\n14.81\n4.99\n4.86\n4.23\n6.34\n1.24\n1.09\n0.77\n0.49\n0.19\n0.09\n—\n\n\n1\n32.90\n28.70\n14.20\n4.70\n4.50\n4.30\n5.9\n1.4\n1.0\n1.7\n0.4\n0.3\n0.1\n—\n\n\n2\n29.00\n25.00\n12.00\n7.00\n7.00\n6.00\n4\n2\n2\n2\n1\n0\n0\n—\n\n\n3\n31.10\n25.30\n10.80\n6.20\n4.60\n5.70\n3.5\n1.5\n1.9\n1.4\n0.4\n0.2\n0.2\n—\n\n\n4\n31.50\n26.60\n11.70\n7.50\n4.40\n6.70\n4.1\n1.8\n2.4\n2.0\n—\n—\n—\n1.3\n\n\n5\n28.00\n24.00\n13.00\n6.00\n6.00\n5.00\n5\n2\n1\n2\n1\n1\n0\n—\n\n\n6\n34.20\n26.90\n13.10\n6.40\n5.40\n4.90\n4.3\n1.7\n1.2\n0.9\n0.6\n0.2\n0.2\n—\n\n\n7\n32.40\n24.80\n11.80\n8.10\n4.40\n5.60\n2.6\n2.1\n1.0\n0.9\n—\n—\n—\n0.7\n\n\n8\n29.70\n25.00\n10.50\n6.10\n5.50\n6.00\n5.4\n1.7\n1.3\n1.5\n0.5\n0.7\n0.0\n—\n\n\n9\n32.60\n26.40\n10.80\n5.30\n3.40\n6.00\n2.7\n1.6\n1.5\n1.0\n0.2\n0.2\n0.2\n—\n\n\n\n\n\n\n\nZauważam, że dla niektórych wartości mamy - oznaczające 0%, zamieńy na wartości numeryczne\n\n# Zamiana myślników (—) na 0 i konwersja na float\ndf = df.replace({'—': 0, '&lt;0.5': 0.25})\ndf = df.astype(float)\ndf.head(10)\n\n\n\n\n\n\n\n\nTrzaskowski\nNawrocki\nMentzen\nHołownia\nZandberg\nBiejat\nBraun\nStanowski\nSenyszyn\nJakubiak\nBartoszewicz\nMaciak\nWoch\nPozostali\n\n\n\n\n0\n31.36\n29.54\n14.81\n4.99\n4.86\n4.23\n6.34\n1.24\n1.09\n0.77\n0.49\n0.19\n0.09\n0.0\n\n\n1\n32.90\n28.70\n14.20\n4.70\n4.50\n4.30\n5.90\n1.40\n1.00\n1.70\n0.40\n0.30\n0.10\n0.0\n\n\n2\n29.00\n25.00\n12.00\n7.00\n7.00\n6.00\n4.00\n2.00\n2.00\n2.00\n1.00\n0.00\n0.00\n0.0\n\n\n3\n31.10\n25.30\n10.80\n6.20\n4.60\n5.70\n3.50\n1.50\n1.90\n1.40\n0.40\n0.20\n0.20\n0.0\n\n\n4\n31.50\n26.60\n11.70\n7.50\n4.40\n6.70\n4.10\n1.80\n2.40\n2.00\n0.00\n0.00\n0.00\n1.3\n\n\n5\n28.00\n24.00\n13.00\n6.00\n6.00\n5.00\n5.00\n2.00\n1.00\n2.00\n1.00\n1.00\n0.00\n0.0\n\n\n6\n34.20\n26.90\n13.10\n6.40\n5.40\n4.90\n4.30\n1.70\n1.20\n0.90\n0.60\n0.20\n0.20\n0.0\n\n\n7\n32.40\n24.80\n11.80\n8.10\n4.40\n5.60\n2.60\n2.10\n1.00\n0.90\n0.00\n0.00\n0.00\n0.7\n\n\n8\n29.70\n25.00\n10.50\n6.10\n5.50\n6.00\n5.40\n1.70\n1.30\n1.50\n0.50\n0.70\n0.00\n0.0\n\n\n9\n32.60\n26.40\n10.80\n5.30\n3.40\n6.00\n2.70\n1.60\n1.50\n1.00\n0.20\n0.20\n0.20\n0.0\n\n\n\n\n\n\n\nWarunkiem zostosowania modelu ARMA jest stacjonarność szeregu czasowego (czyli średnia wartośc nie powinna być zmienna w czasie). Niektóre szeregi wyników sondażowych kandydatów są stacjonarne, a niektóre nie. Można to sprawdzić za pomocą testu ADF. Jeśli p-value &lt; 0.05 to odrzucamy hipoteze zerową o niestacjonarności szeregu. Gdy szereg jest niestacjonarny to trzeba go zróznicować i na szeregu różnic dokonać analizy ARMA.\n\n# Na podstawie eksperymentów na danych historycznych wybrałem hiperparametry ARMA(2,2) \np = 2\nq = 2\nout = {}\nfor column in df.columns:\n    # Szereg czasowy wyników sondażowych, w danych jest od najnowszego, a dla ARMA potrzebujemy od najstarczego\n    wartosci = df[column].values[::-1]\n    # Test ADF\n    result = adfuller(wartosci)\n    p_value = result[1]\n    \n    if p_value &lt; 0.05:\n        # Szereg stacjonarny\n        model = ARIMA(wartosci, order=(p, 0, q))\n        model_fit = model.fit()\n        # Prognoza dla następnego okresu, tj. wyborów.\n        forecast = model_fit.forecast(steps=1)[0]\n        out[column] = forecast\n    else:\n        # Szereg niestacjonarny\n        zroznicowane_wartosci = np.diff(wartosci)\n        model = ARIMA(zroznicowane_wartosci, order=(p, 0, q))\n        model_fit = model.fit()\n        # Tu nie przywidujemy wyniku, a wartość róznicy wyniku w wyborach i ostatniego wyniku w sondażu. \n        forecast = model_fit.forecast(steps=1)[0]\n        ostatnia_wartosc = wartosci[-1]\n        out[column] = forecast + ostatnia_wartosc\n\nZobaczmy wyniki.\n\ny_pred = pd.Series(out)\ny_pred\n\nTrzaskowski     31.000594\nNawrocki        24.691635\nMentzen         12.866129\nHołownia         6.392249\nZandberg         5.650612\nBiejat           5.688687\nBraun            3.941419\nStanowski        1.616397\nSenyszyn         1.615240\nJakubiak         1.402906\nBartoszewicz     0.418326\nMaciak           0.284570\nWoch             0.055905\nPozostali        0.399684\ndtype: float64\n\n\nA co z kandydatami nieuwzględnionymi w sondażach na ewyboru.eu? Pozwolę sobie dopisać ich do powyższego wektora “z palca”. Według ewybory.wu ich szanse prezentują się następująco:\n\n# W procentacj\nout[\"Maciak\"] = 0.2 # 0.2%\nout[\"Bartosiewicz\"] = 0.2\nout[\"Woch\"] = 0.1\n\ny_pred = pd.Series(out)\ny_pred\n\nTrzaskowski     31.000594\nNawrocki        24.691635\nMentzen         12.866129\nHołownia         6.392249\nZandberg         5.650612\nBiejat           5.688687\nBraun            3.941419\nStanowski        1.616397\nSenyszyn         1.615240\nJakubiak         1.402906\nBartoszewicz     0.418326\nMaciak           0.200000\nWoch             0.100000\nPozostali        0.399684\nBartosiewicz     0.200000\ndtype: float64\n\n\nWyniki nie sumują się do 100%, teraz znormalizuje dane, aby tak było.\n\nsum(y_pred)\n\n96.18387837228907\n\n\n\n# Ograniczenie do przedziału [0, 100], gdyby jakiaś predykcja była poniżej 0 \ny_pred = np.clip(y_pred, 0, 100)\n# Normalizacja tak, by suma = 100%\ny_pred_normalized = 100 * y_pred / y_pred.sum()\n\n\ny_pred_normalized\n\nTrzaskowski     32.230551\nNawrocki        25.671283\nMentzen         13.376596\nHołownia         6.645863\nZandberg         5.874802\nBiejat           5.914387\nBraun            4.097796\nStanowski        1.680528\nSenyszyn         1.679325\nJakubiak         1.458567\nBartoszewicz     0.434923\nMaciak           0.207935\nWoch             0.103968\nPozostali        0.415541\nBartosiewicz     0.207935\ndtype: float64\n\n\n\n# Formatownie, na stornie PKW procenty są do dwóch miejsc po przecinku\nprocenty = y_pred_normalized.sort_values(ascending=False).apply(lambda x: f\"{x:.2f}%\")\nprint(procenty)\n\nTrzaskowski     32.23%\nNawrocki        25.67%\nMentzen         13.38%\nHołownia         6.65%\nBiejat           5.91%\nZandberg         5.87%\nBraun            4.10%\nStanowski        1.68%\nSenyszyn         1.68%\nJakubiak         1.46%\nBartoszewicz     0.43%\nPozostali        0.42%\nMaciak           0.21%\nBartosiewicz     0.21%\nWoch             0.10%\ndtype: object"
  },
  {
    "objectID": "Projekty/GA/index.html",
    "href": "Projekty/GA/index.html",
    "title": "4. Algorytm genetyczny",
    "section": "",
    "text": "Projekt wykonany w Pythonie na zajęcia Credit Scoring.\nPodstawowym zagadnieniem było zbudowanie modelu do oceny ryzyka potencjalnych klientów kredytowych za pomocą kodów udostępnionych przez wykładowcę, pana dr Karola Przanowskiego. Dla chętnych była możliowość ich udoskonalenia wg własnej inwencji twórczej. Ja zdecydowałem się zaimplementować algorytm genetyczny do wyboru zmiennych modelu.\nDo wyboru w danych mieliśmy ok. 200 zmiennych, podczas gdy do modelu szukaliśmy najlepszej kombinacji kilku/kilkunastu. Funkcją celu było znalezienie modelu z jak największym indeksem Gini, mierzącym jego siłę predykcyjną.\nPoniżej zamieszczam kod, którego jestem autorem. Cały projekt, włącznie z kodem profesora, można znaleźc tutaj."
  },
  {
    "objectID": "Projekty/RWrapped/index.html",
    "href": "Projekty/RWrapped/index.html",
    "title": "1. R Wrapped",
    "section": "",
    "text": "Projekt wykonany w języku R w ramach zajęć Prezentacja i wizualizacja danych."
  },
  {
    "objectID": "Projekty/RWrapped/index.html#wprowadzenie",
    "href": "Projekty/RWrapped/index.html#wprowadzenie",
    "title": "1. R Wrapped",
    "section": "Wprowadzenie",
    "text": "Wprowadzenie\nLubię słuchać muzyki. Łącząc to z moim naturalnym zamiłowaniem do analizy danych, od pewnego czasu dokładnie analizuję jakiej muzyki słucham.\nNie jestem jedynym, który miał taki pomysł. Dla osób, które chcą wiedzieć, czego dokładnie słuchają powstała strona &lt;last.fm&gt;. Tam, po połączeniu z kontem Spotify, można dokładnie zobaczyć czego i o której godzinie słuchano.\nIstnieje nawet biblioteka w R autorstwa Piotra Patrzyka, która pozwala załadować dane ze swojego konta na last.fm w postaci data.frame w R\nDzięki tej bibliotece mogłem rozpocząć analizę.\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(lubridate)\nlibrary(remotes)\n#remotes::install_github(\"ppatrzyk/lastfmR\")\nlibrary(lastfmR)\n\nPodstawową funkcją tej biblioteki jest get_scrobbles.\n\nuser &lt;- \"c00lll\"\nscrobbles &lt;- get_scrobbles(user = user, timezone = 'Europe/Warsaw')\n\n\nhead(scrobbles,10)\n\n                   date           artist                            track\n               &lt;POSIXt&gt;           &lt;char&gt;                           &lt;char&gt;\n 1: 2025-05-21 11:36:36          YULLOLA      The Girl Who Lost the World\n 2: 2025-05-21 11:33:56       Baby Gravy  C'est La Vie (feat. Rich Brian)\n 3: 2025-05-21 11:20:09     Lana Del Rey Chemtrails Over the Country Club\n 4: 2025-05-21 11:16:20       Mello Crum                Memories of Color\n 5: 2025-05-21 11:09:15 Yngwie Malmsteen                             Fire\n 6: 2025-05-21 11:06:52        Pentagram                 Forever My Queen\n 7: 2025-05-21 11:03:42        Lita Ford            Close My Eyes Forever\n 8: 2025-05-21 10:55:31       Charli XCX                        party 4 u\n 9: 2025-05-21 10:50:32              Dio              Rainbow in the Dark\n10: 2025-05-21 10:45:19    Ozzy Osbourne                      Mr. Crowley\n                               album\n                              &lt;char&gt;\n 1:                Monastery of Love\n 2:                     Baby Gravy 3\n 3: Chemtrails Over the Country Club\n 4:                Memories of Color\n 5:                          Trilogy\n 6:        First Daze Here (Reissue)\n 7:                             Lita\n 8:              how i'm feeling now\n 9:                       Holy Diver\n10:      The Essential Ozzy Osbourne\n\n\nSortuje dane tak, aby najstarszy odsłuch był na górze. Potem zliczam, po raz który dany album/piosenka/artysta został odsłuchany. To przyda mi się później.\n\nscrobbles &lt;- scrobbles[(order(as.Date(scrobbles$date, format=\"%Y-%m-%d %H:%M:%S\"))),]\nscrobbles &lt;- scrobbles[rev(order(as.Date(scrobbles$date, format=\"%Y-%m-%d %H:%M:%S\"))),]\nscrobbles &lt;- scrobbles[(order(as.Date(scrobbles$date, format=\"%Y-%m-%d %H:%M:%S\"))),]\nscrobbles$jedynki &lt;- 1\nscrobbles$cum_sum &lt;- ave(scrobbles$jedynki, scrobbles$track, FUN=cumsum)\nscrobbles$cum_sum_artists &lt;- ave(scrobbles$jedynki, scrobbles$artist, FUN=cumsum)\nscrobbles$cum_sum_albums &lt;- ave(scrobbles$jedynki, scrobbles$album, FUN=cumsum)\nhead(scrobbles,10)\n\n                   date       artist  track         album jedynki cum_sum\n               &lt;POSIXt&gt;       &lt;char&gt; &lt;char&gt;        &lt;char&gt;   &lt;num&gt;   &lt;num&gt;\n 1: 2018-06-18 14:46:17 BROCKHAMPTON  JUNKY SATURATION II       1       1\n 2: 2018-06-18 14:51:57 BROCKHAMPTON   BANK    SATURATION       1       1\n 3: 2018-06-18 14:55:13 BROCKHAMPTON   TRIP    SATURATION       1       1\n 4: 2018-06-18 14:58:37 BROCKHAMPTON   STAR    SATURATION       1       1\n 5: 2018-06-18 15:01:18 BROCKHAMPTON   FAKE    SATURATION       1       1\n 6: 2018-06-18 22:47:18 BROCKHAMPTON   BANK    SATURATION       1       2\n 7: 2018-06-18 22:50:51 BROCKHAMPTON   TRIP    SATURATION       1       2\n 8: 2018-06-18 22:54:30 BROCKHAMPTON   STAR    SATURATION       1       2\n 9: 2018-06-18 22:56:05 BROCKHAMPTON   DIRT          Dirt       1       1\n10: 2018-06-19 00:00:01 BROCKHAMPTON   DIRT          Dirt       1       2\n    cum_sum_artists cum_sum_albums\n              &lt;num&gt;          &lt;num&gt;\n 1:               1              1\n 2:               2              1\n 3:               3              2\n 4:               4              3\n 5:               5              4\n 6:               6              5\n 7:               7              6\n 8:               8              7\n 9:               9              1\n10:              10              2"
  },
  {
    "objectID": "Projekty/RWrapped/index.html#listy-top-10",
    "href": "Projekty/RWrapped/index.html#listy-top-10",
    "title": "1. R Wrapped",
    "section": "Listy Top 10",
    "text": "Listy Top 10\nUdało się! Teraz mogę wykonać analizy. Na początek zobaczmy, jakie jest 10 piosenek z największą liczbą odtworzeń.\n\n#Sortowanie według liczby odsłuchań piosenek\ncount_whole &lt;- table(scrobbles$track)\ncount_whole_sorted &lt;- sort(count_whole, decreasing = TRUE)\n#10 najczęściej odtwarzanych utworów\ntop10_tracks_sorted &lt;- head(count_whole_sorted, 10)\n\n#wyświetlenie 10 najczęściej odtwarzanych piosenek\nfor (i in 1:10) {\n  cat(paste0(i, \". \", scrobbles[track == names(top10_tracks_sorted[i])]$artist[2], \" - \", names(top10_tracks_sorted[i]),\": \", as.numeric(top10_tracks_sorted[i]), \"\\n\"))\n}\n\n1. 100 gecs - money machine: 249\n2. Meek, Oh Why? - Pieśniarka i Król: 246\n3. King Krule - Czech One: 209\n4. JPEGMAFIA - 1539 N. Calvert: 207\n5. 100 gecs - hand crushed by a mallet: 190\n6. Coals - Blue (feat. Schafter): 166\n7. 100 gecs - ringtone: 158\n8. King Krule - Dum Surfer: 150\n9. laura les - Haunted: 150\n10. 100 gecs - sympathy 4 the grinch: 150\n\nfor (i in 1:10) {\n  names(top10_tracks_sorted)[i] &lt;- paste0(scrobbles[track == names(top10_tracks_sorted[i])]$artist[2], \" - \", names(top10_tracks_sorted[i]))\n}\n\nZaprezentujmy to w formie jakiegoś wykresu.\n\npar(mar=c(5,2,2,2))\npal &lt;- colorRamp(c(\"#050861\", \"#131ad6\"))\nvalues_barplot &lt;- as.numeric(rev(top10_tracks_sorted))\nkolor_barplot &lt;- rgb(pal((values_barplot - min(values_barplot))/ diff(range(values_barplot))), max=255)\nbarplot_tracks &lt;- barplot(rev(top10_tracks_sorted), horiz = T, las = 1, yaxt = \"n\", xlim=c(0, max(scrobbles$cum_sum)), col = kolor_barplot, xlab = \"Ilość odtworzeń\", main = \"Top 10 utworów wszechczasów\")\ntext(as.numeric(rev(top10_tracks_sorted)), barplot_tracks, labels = as.numeric(rev(top10_tracks_sorted)), pos = 2, col = \"white\", cex = 3/4, font = 2)\ntext(1, barplot_tracks, labels = paste0(rev(names(top10_tracks_sorted))), pos = 4, col = \"white\", cex = 0.75, font = 2)\n\n\n\n\n\n\n\n\nAnalogiczną analizę przeprowadzono dla artystów i albumów.\n\ncount_artist &lt;- table(scrobbles$artist)\nartist_sorted &lt;- sort(count_artist, decreasing = T)\n#10 najczęściej odtwarzanych artystów\ntop10_artist_sorted &lt;- head(artist_sorted,10)\nfor (i in 1:10) {\n  cat(i, \". \", names(top10_artist_sorted[i]),\": \", as.numeric(top10_artist_sorted[i]), \"\\n\", sep = \"\")\n}\n\n1. Taco Hemingway: 3029\n2. 100 gecs: 2512\n3. JPEGMAFIA: 2205\n4. Holak: 2117\n5. Meek, Oh Why?: 1950\n6. Charli XCX: 1917\n7. BROCKHAMPTON: 1717\n8. TV Girl: 1472\n9. Kendrick Lamar: 1391\n10. King Krule: 1206\n\n\n\n#Wykres 10 najczęściej odtwarzanych artystów\npar(mar=c(5,2,2,2))\ntop10_artist_sorted &lt;- head(artist_sorted,10)\npal &lt;- colorRamp(c(\"#4f1403\", \"#992c0e\"))\nvalues_barplot &lt;- as.numeric(rev(top10_artist_sorted))\nkolor_barplot &lt;- rgb(pal((values_barplot - min(values_barplot))/ diff(range(values_barplot))), max=255)\nbarplot_tracks &lt;- barplot(rev(top10_artist_sorted), horiz = T, las = 1, yaxt = \"n\", col = kolor_barplot, xlab = \"Ilość odtworzeń\", main = \"Top 10 artystów wszechczasów\")\ntext(as.numeric(rev(top10_artist_sorted)), barplot_tracks, labels = as.numeric(rev(top10_artist_sorted)), pos = 2, col = \"white\", cex = 0.8, font = 2)\ntext(1, barplot_tracks, labels = paste0(rev(names(top10_artist_sorted))), pos = 4, col = \"white\", cex = ((-0.2/3483)*(as.numeric(top10_artist_sorted[1] - top10_artist_sorted[10]) ) + 0.8), font = 2)\n\n\n\n\n\n\n\n\n\ncount_album &lt;- table(scrobbles$album)\nalbums_sorted &lt;- sort(count_album, decreasing = T)\n#10 najczęściej odtwarzanych albumów\ntop10_albums_sorted &lt;- head(albums_sorted,10)\nfor (i in 1:10) {\n  cat(i, \". \", scrobbles[album == names(top10_albums_sorted[i])]$artist[2], \" - \", names(top10_albums_sorted[i]),\": \", as.numeric(top10_albums_sorted[i]), \"\\n\",sep = \"\")\n}\n\n1. Meek, Oh Why? - Zachód: 1130\n2. 100 gecs - 1000 gecs: 881\n3. King Krule - The OOZ: 768\n4. Travis Scott - ASTROWORLD: 652\n5. 100 gecs - 1000 gecs and The Tree of Clues: 639\n6. BROCKHAMPTON - GINGER: 614\n7. Taco Hemingway - Marmur: 590\n8. Taco Hemingway - Café Belga: 571\n9. Phoebe Bridgers - Punisher: 563\n10. Taco Hemingway - Pocztówka z WWA, lato '19: 556\n\nfor (i in 1:10) {\n  names(top10_albums_sorted)[i] &lt;- paste0(scrobbles[album == names(top10_albums_sorted[i])]$artist[2], \" - \", names(top10_albums_sorted[i]))\n}\n\n\npar(mar=c(5,2,2,2))\npal &lt;- colorRamp(c(\"#004f1c\", \"#0fa343\"))\nvalues_barplot &lt;- as.numeric(rev(top10_albums_sorted))\nkolor_barplot &lt;- rgb(pal((values_barplot - min(values_barplot))/ diff(range(values_barplot))), max=255)\nbarplot_tracks &lt;- barplot(rev(top10_albums_sorted), horiz = T, las = 1, yaxt = \"n\", col = kolor_barplot, xlab = \"Ilość odtworzeń\", main = \"Top 10 albumów wszechczasów\")\ntext(as.numeric(rev(top10_albums_sorted)), barplot_tracks, labels = as.numeric(rev(top10_albums_sorted)), pos = 2, col = \"white\", cex = 0.8, font = 2)\ntext(1, barplot_tracks, labels = paste0(rev(names(top10_albums_sorted))), pos = 4, col = \"white\", cex = 0.7, font = 2)"
  },
  {
    "objectID": "Projekty/RWrapped/index.html#ewolucja-liczby-odsłuchań-utworów",
    "href": "Projekty/RWrapped/index.html#ewolucja-liczby-odsłuchań-utworów",
    "title": "1. R Wrapped",
    "section": "Ewolucja liczby odsłuchań utworów",
    "text": "Ewolucja liczby odsłuchań utworów\nPowyższe wykresy przedstawiają podsumowanie całej mojej historii słuchacza. Natomiast moje preferencje z czasem się zmieniały. Rozpoczynając ten projekt, właśnie to chciałem zwizualizować.\nZobaczmy, jak zmieniali się liderzy w klasyfikacji odsłuchań.\n\n#############\n#Ewolucja liczby odsłuchań utworów\ntop_track &lt;- scrobbles[track == names(count_whole_sorted[1]),]\n\nplot(top_track$date, top_track$cum_sum, type = \"s\", col = \"gold\", lwd=3, xlim = c(min(scrobbles$date), max(scrobbles$date)), xlab = \"Czas odsłuchu\", ylab = \"Ilość odtworzeń\", main = \"Ewolucja liczby odtworzeń utworów\")\n\nfor (track_name in names(count_whole_sorted[1:1000])) {\n  to_draw &lt;- scrobbles[track == track_name,]\n  lines(to_draw$date, to_draw$cum_sum, type = \"s\", col = \"#dbd7ca\")\n}\n\nlines(top_track$date, top_track$cum_sum, type = \"s\", col = \"gold\", lwd=3)\n\nmy_range &lt;- 4:10\nfor (i in my_range) {\n  to_draw &lt;- scrobbles[track == names(count_whole_sorted[i]),]\n  lines(to_draw$date, to_draw$cum_sum, type = \"s\", col = \"black\")\n}\n\ntop_track3 &lt;- scrobbles[track == names(count_whole_sorted[3]),]\nlines(top_track3$date, top_track3$cum_sum, type = \"s\", col = \"brown\", lwd = 3)\n\ntop_track2 &lt;- scrobbles[track == names(count_whole_sorted[2]),]\nlines(top_track2$date, top_track2$cum_sum, type = \"s\", col = \"grey\", lwd = 3)\n\nlines(top_track$date, top_track$cum_sum, type = \"s\", col = \"gold\", lwd=3)\n\n\nlegend(min(scrobbles$date), max(scrobbles$cum_sum), c(names(count_whole_sorted[1]), names(count_whole_sorted[2]), names(count_whole_sorted[3])), col=c(\"gold\", \"grey\", \"brown\"),lty = 1, cex=0.6, title=\"Nazwy utworów\", text.font=1) \n\n\n\n\n\n\n\n\nNajbardziej zaciekawił mnie trend, w którym wszystkie piosenki mają wielki wyskok na początku, po którym następuje zwolnienie. Jest to jak najbardziej spodziewane, ponieważ po pewnym czasie piosenki tracą na “świeżości” i przechodzę do kolejnej.\n\n#############\n#Ewolucja liczby odtworzeń artystów\ntop_artist &lt;- scrobbles[artist == names(artist_sorted[1]),]\n\nplot(top_artist$date, top_artist$cum_sum_artists, type = \"s\", col = \"gold\", lwd=3, xlim = c(min(scrobbles$date), max(scrobbles$date)), xlab = \"Czas odsłuchu\", ylab = \"Ilość odtworzeń\", main = \"Ewolucja liczby odtworzeń artystów\")\n\n\nfor (artist_name in names(artist_sorted[1:1000])) {\n  to_draw &lt;- scrobbles[artist == artist_name,]\n  lines(to_draw$date, to_draw$cum_sum_artists, type = \"s\", col = \"#dbd7ca\")\n}\n\nmy_range &lt;- 4:10\nfor (i in my_range) {\n  to_draw &lt;- scrobbles[artist == names(artist_sorted[i]),]\n  lines(to_draw$date, to_draw$cum_sum_artists, type = \"s\", col = \"black\")\n}\n\ntop_artist3 &lt;- scrobbles[artist  == names(artist_sorted[3]),]\nlines(top_artist3$date, top_artist3$cum_sum_artists, type = \"s\", col = \"brown\", lwd = 3)\n\ntop_artist2 &lt;- scrobbles[artist  == names(artist_sorted[2]),]\n\nlines(top_artist2$date, top_artist2$cum_sum_artists, type = \"s\", col = \"grey\", lwd = 3)\nlines(top_artist$date, top_artist$cum_sum_artists, type = \"s\", col = \"gold\", lwd=3)\n\n\n\nlegend(min(scrobbles$date), max(scrobbles$cum_sum_artists), c(names(artist_sorted[1]), names(artist_sorted[2]), names(artist_sorted[3])), col=c(\"gold\", \"grey\", \"brown\"),lty = 1, cex=0.6, title=\"Nazwy artystów\", text.font=1) \n\n\n\n\n\n\n\n\n\n#####\n#Ewolucja liczby odtworzeń albumów\ntop_album &lt;- scrobbles[album == names(albums_sorted[1]),]\n\nplot(top_album$date, top_album$cum_sum_albums, type = \"s\", col = \"gold\", lwd=3, xlim = c(min(scrobbles$date), max(scrobbles$date)), xlab = \"Data odsłuchu\", ylab = \"Ilość odtworzeń\", main = \"Ewolucja liczby odtworzeń albumów\")\n\nfor (album_name in names(albums_sorted[1:1000])) {\n  to_draw &lt;- scrobbles[album == album_name,]\n  lines(to_draw$date, to_draw$cum_sum_albums, type = \"s\", col = \"#dbd7ca\")\n}\n\n\nmy_range &lt;- 4:10\nfor (i in my_range) {\n  to_draw &lt;- scrobbles[album == names(albums_sorted[i]),]\n  lines(to_draw$date, to_draw$cum_sum_albums, type = \"s\", col = \"black\")\n}\n\n\ntop_album3 &lt;- scrobbles[album  == names(albums_sorted[3]),]\nlines(top_album3$date, top_album3$cum_sum_albums, type = \"s\", col = \"brown\", lwd = 3)\n\ntop_album2 &lt;- scrobbles[album  == names(albums_sorted[2]),]\nlines(top_album2$date, top_album2$cum_sum_albums, type = \"s\", col = \"grey\", lwd = 3)\n\nlines(top_album$date, top_album$cum_sum_albums, type = \"s\", col = \"gold\", lwd=3)\n\n\nlegend(min(scrobbles$date), max(scrobbles$cum_sum_albums), c(names(albums_sorted[1]), names(albums_sorted[2]), names(albums_sorted[3])), col=c(\"gold\", \"grey\", \"brown\"),lty = 1, cex=0.7, title=\"Tytuły albumów\", text.font=1)"
  },
  {
    "objectID": "Projekty/RWrapped/index.html#ggplot2",
    "href": "Projekty/RWrapped/index.html#ggplot2",
    "title": "1. R Wrapped",
    "section": "ggplot2",
    "text": "ggplot2\nInną rzeczą, którą chciałem sprawdzić jest to jak zmieniały się moje nawyki melomana podczas okresu zbierania danych. Czy teraz słucham więcej muzyki niż wcześniej?\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\nscrobbles_copy &lt;- scrobbles\n\ntibble_months &lt;- (scrobbles_copy  %&gt;% \n         group_by(month = lubridate::floor_date(date, 'month')) %&gt;%\n         summarize(l_wystapien = sum(jedynki)))\n\ntibble_months$month_name &lt;- month(tibble_months$month)\ntibble_months$year  &lt;- year(tibble_months$month)\n\nfirst_year &lt;- tibble_months$year |&gt; head(1)\ncurrent_year &lt;- tibble_months$year |&gt; tail(1)\n# Heatmap \nggplot(tibble_months, aes(year, month_name)) + \n  geom_tile(aes(fill= l_wystapien), col = \"white\") +\n  scale_fill_gradient(low = \"#fac8d8\", high = \"#87002b\", name = \"Odtworzenia\") +\n  ggtitle(\"Liczba odsłuchań danego miesiąca\") +\n  scale_y_reverse() + scale_x_discrete(limits=first_year:current_year)+\n  xlab(\"Rok\") + ylab(\"Miesiąc\")\n\n\n\n\n\n\n\n#heatmap z wartosciami\nggplot(tibble_months, aes(year, month_name)) + \n  geom_tile(aes(fill= l_wystapien), col = \"white\") +\n  scale_fill_gradient(low = \"#fac8d8\", high = \"#87002b\", name = \"Odtworzenia\") +\n  ggtitle(\"Liczba odsłuchań danego miesiąca\") +\n  scale_x_discrete(limits=first_year:current_year)+\n  xlab(\"Rok\") + ylab(\"Miesiąc\") + \n  geom_text(aes(label = l_wystapien), col = \"white\", cex = 4.5) +\n  scale_y_reverse()\n\n\n\n\n\n\n\n\nJednak więcej słuchałem kiedyś… Co ciekawe po rozpoczęciu pandemii w marcu 2020 roku liczba odtworzeń się drastycznie zmniejszyła. Można się było spodziewać, że miałem więcej wolnego czasu na słuchanie muzyki, lecz to jednak słuchanie muzyki w drodze do szkoły czy pracy skutkowało największą liczbą odsłuchań.\nInnym pytaniem jest, co wydarzyło się w czerwcu 2024 roku? Był to dla mnie intensywny okres sesji, podczas którego wspomagałem się muzyką. Natomiast rozmiar tego wsparcia mocno mnie zaskoczył!\nJacy są najczęsciej słuchani artyści w każdym miesiącu?\n\n###  najpopularniejsi artyści każdego miesiaca\nlibrary(zoo)\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:data.table':\n\n    yearmon, yearqtr\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\nfirst_scrobble_date &lt;- scrobbles$date[1]\nn_months &lt;- 1 + interval(as.yearmon(first_scrobble_date), today()) %/% months(1)\ndetach(\"package:zoo\", unload = TRUE)\n# wiersze to miesiące, kolumny to lata\n# Pierwszy rok, nie musiał się zacząć w styczniu\nfirst_scrobble_date_month &lt;- month(first_scrobble_date)\n\nif ((n_months - (12 - first_scrobble_date_month + 1)) %% 12 != 0) {\nliczbLat &lt;- ceiling((n_months - (12 - first_scrobble_date_month + 1))/12) + 1\n} else {\nliczbLat &lt;- ceiling((n_months - (12 - first_scrobble_date_month + 1))/12)  \n}\n\n\nmacierz &lt;- matrix(NA, nrow = 12, ncol = liczbLat)\nmacierz_to_plot &lt;- matrix(NA, n_months, 2)\ndate_temp &lt;- min(scrobbles_copy$date)\ndate_temp &lt;- floor_date(date_temp, 'month')\ndate_temp_next &lt;- date_temp %m+% months(1)\npier_mies = first_scrobble_date_month\nfor (i in 1:n_months) {\n  month_in_scr &lt;- scrobbles[scrobbles$date &gt; date_temp & scrobbles$date &lt; date_temp_next]\n  t_month_in_scr &lt;- table(month_in_scr$artist)\n  t_month_in_scr &lt;- sort(t_month_in_scr, decreasing = T)\n  naj_artysta &lt;- names(t_month_in_scr[1])\n  if (is.null(naj_artysta)) {\n    naj_artysta = \"No data for this month\"\n    ich_odtworzenia_wtedy = 0\n    } else {\n  ich_odtworzenia_wtedy &lt;- as.numeric(t_month_in_scr[1])\n    }\n  date_temp_next &lt;- date_temp_next %m+% months(1)\n  date_temp &lt;- date_temp %m+% months(1)\n  macierz[pier_mies] = naj_artysta\n  macierz_to_plot[pier_mies-first_scrobble_date_month+1, 1] = substr(naj_artysta,1,26)\n  macierz_to_plot[pier_mies-first_scrobble_date_month+1, 2] = ich_odtworzenia_wtedy\n  pier_mies &lt;- pier_mies + 1\n}\n#macierz\n#macierz_to_plot\n\nmacierz &lt;- matrix(macierz, nrow = 12, ncol = ceiling(length(macierz)/12))\nif (year(scrobbles$date) %&gt;% unique() %&gt;% length() != 1){\n  colnames(macierz) &lt;- year(first_scrobble_date):year(tail(scrobbles$date,1))\n  rownames(macierz) &lt;- c(\"Styczeń\", \"Luty\", \"Marzec\", \"Kwiecień\", \"Maj\", \"Czerwiec\", \"Lipiec\", \"Sierpień\", \"Wrzesień\", \"Październik\", \"Listopad\", \"Grudzień\")\n}\n\n\ndf_barplot &lt;- as.data.frame(macierz_to_plot)\n\n## To add, if len(df_barplot) != len(tibble_months), go by row in tibble months and add missing months\ndf_barplot$month_name &lt;- c(tibble_months$month_name)\ndf_barplot$year &lt;- c(tibble_months$year)\n\nnames(df_barplot)[1:2] &lt;- c(\"Artysta\", \"Wystapienia_w_mies\")\n\ntibble_barplot &lt;- as_tibble(df_barplot)\ntibble_barplot$Wystapienia_w_mies &lt;- as.double(tibble_barplot$Wystapienia_w_mies)\n\ng &lt;- ggplot(tibble_barplot, aes(year, as.numeric(month_name))) + \n  geom_tile(aes(fill= as.double(Wystapienia_w_mies)), col = \"white\") +\n  scale_fill_gradient(low = \"#e5ced4\", high = \"#451350\", name = \"Odtworzenia \\n w miesiącu\") +\n  ggtitle(\"Liczba odsłuchań najczęściej słuchanego artysty danego miesiąca\") +\n  scale_x_discrete(limits=year(first_scrobble_date):year(tail(scrobbles$date,1)))+\n  xlab(\"Rok\") + ylab(\"Miesiąc\") +\n  scale_y_reverse() +\n  geom_text(aes(label = substr(Artysta, 1, 14)), col = \"white\", font = 2, size = 2.5)\ng\n\n\n\n\n\n\n\n\nDzięki temu wykresowi możemy zobaczyć, że do wysokiego wyniku w czerwcu 2024 przyczyniła się premiera nowego albumu Charli XCX pt. “brat”\nSpójrzmy w jakich miesiącach roku, słuchałem najwięcej.\n\n#Średnia liczba odtworzeń na miesiąc\nsrednie_wartosci &lt;- tibble_months %&gt;%\n  group_by(month_name) %&gt;%\n  summarise(SredniaWartosc = mean(l_wystapien))\n\nsrednie_wartosci &lt;- setNames(srednie_wartosci, c(\"Miesiąc\", \"Średnia Liczba odtworzeń na miesiąc\"))\nsrednie_wartosci$Miesiąc &lt;- factor(month.name[srednie_wartosci$Miesiąc], levels = month.name)\n\n\npar(mar=c(5,6,3,3))\npal &lt;- colorRamp(c(\"#37420D\", \"#9CBD26\"))\nvalues_barplot &lt;- as.numeric(rev(srednie_wartosci$`Średnia Liczba odtworzeń na miesiąc`))\nkolor_barplot &lt;- rgb(pal((values_barplot - min(values_barplot))/ diff(range(values_barplot))), max=255)\nbarplot_mies &lt;- barplot(rev(srednie_wartosci$`Średnia Liczba odtworzeń na miesiąc`), names = rev(srednie_wartosci$Miesiąc), las = 1, xlim=c(0,max(srednie_wartosci$`Średnia Liczba odtworzeń na miesiąc`)), horiz = T, col = kolor_barplot, xlab = \"Ilość odtworzeń\", main = \"Średnia liczba odtworzeń w miesiącu\")\ntext(((max(srednie_wartosci$`Średnia Liczba odtworzeń na miesiąc`))/30), barplot_mies, labels = round(rev(srednie_wartosci$`Średnia Liczba odtworzeń na miesiąc`)), col = \"white\", font = 2, cex = 0.9)\n\n\n\n\n\n\n\n\nSpójrzmy też, w jakich latach słuchałem najwięcej.\n\nby_year &lt;- aggregate(l_wystapien ~ year, data=tibble_months, sum)\n#Todo: if a year is missing, add it and count 0\nby_year &lt;- setNames(by_year, c(\"Rok\", \"Liczba odtworzeń\"))\n\nbarplot_przed_usr &lt;- barplot(by_year[,2], names = by_year[,1], ylim = c(0,18000), main = \"Liczba odtworzeń w ciągu roku\", col = \"#aa5555\")\ntext(barplot_przed_usr, by_year[,2] + 650, labels = by_year[,2])\n\n\n\n\n\n\n\n#Srednia odtworzen na miesiąc podczas danego roku\nif ((n_months - (12 - first_scrobble_date_month + 1)) %% 12 &gt; 0) {\nby_year$l_miesiecy &lt;-c(12 - first_scrobble_date_month + 1,rep(12, length.out = floor((n_months - (12 - first_scrobble_date_month + 1))/12)),(n_months - (12 - first_scrobble_date_month + 1)) %% 12)\n} else {\n  by_year$l_miesiecy &lt;-c(12 - first_scrobble_date_month + 1,rep(12, length.out = floor((n_months - (12 - first_scrobble_date_month + 1))/12)))\n}\n\n\n\nby_year$na_mies &lt;- by_year$`Liczba odtworzeń` / by_year$l_miesiecy\nby_year$na_mies &lt;- round(by_year$na_mies,0)\n#t(by_year)\npokaz_by_year &lt;- t(by_year[c(1,4)])\n#pokaz_by_year\nbarplot_lata &lt;- barplot(pokaz_by_year[2,], names = pokaz_by_year[1,], col = \"#aa5555\", main = \"Średnia liczba odtworzeń na miesiąc\")\ntext(barplot_lata, pokaz_by_year[2,] + 50, labels = pokaz_by_year[2,])\n\n\n\n\n\n\n\n\nPrzeanalizujmy teraz godziny w ciągu dnia w jakich najczęściej słucham muzyki.\n\nlibrary(lubridate)\n#heat map dzien/godzina\nday_plot &lt;- (scrobbles_copy  %&gt;% \n         group_by(date = lubridate::floor_date(date, 'hour')) %&gt;%\n         summarize(l_wystapien = sum(jedynki)))\n\n#day_plot$day &lt;- wday(day_plot$date, week_start = getOption(\"lubridate.week.start\", 1))\n\nday_plot$day &lt;- wday(day_plot$date - days(1))\nday_plot$hour  &lt;- hour(day_plot$date)\n\nday_plot_to_draw &lt;- day_plot %&gt;%\n  group_by(day, hour) %&gt;%\n  summarise(Suma = sum(l_wystapien))\n\n`summarise()` has grouped output by 'day'. You can override using the `.groups`\nargument.\n\n# Heatmap \ng &lt;- ggplot(day_plot_to_draw, aes(day, hour)) + \n  geom_tile(aes(fill= Suma), col = \"white\") +\n  scale_fill_gradient(low = \"#fad8e8\", high = \"#67000b\", name = \"Odtworzenia\") +\n  ggtitle(\"Liczba odsłuchań o danej porze dnia\")  + scale_x_discrete(limits=1:7, labels = c(\"Poniedziałek\", \"Wtorek\", \"Środa\", \"Czwartek\", \"Piątek\", \"Sobota\", \"Niedziela\")) +\n  scale_y_discrete(limits=0:23) + xlab(\"Dzień tygodnia\") + ylab(\"Godzina\")\ng\n\n\n\n\n\n\n\n#dodanie wartości na mapę\ng + geom_text(aes(label = Suma), col = \"white\", cex = 3)\n\n\n\n\n\n\n\n# dzień i godzina z największą liczbą odsłuchań\nday_plot_to_draw[which.max(day_plot_to_draw$Suma),]\n\n# A tibble: 1 × 3\n# Groups:   day [1]\n    day  hour  Suma\n  &lt;int&gt; &lt;int&gt; &lt;dbl&gt;\n1     2    17  1066\n\n\nKrólują godziny popołudniowe powrotu ze szkoły i pracy. Najwięcej - we wtorek o godz. 17."
  },
  {
    "objectID": "Projekty/RWrapped/index.html#chmura-gatunków",
    "href": "Projekty/RWrapped/index.html#chmura-gatunków",
    "title": "1. R Wrapped",
    "section": "Chmura gatunków",
    "text": "Chmura gatunków\nZobaczmy jakie gatunki najczęściej występują u artystów, których słucham.\n\nartist_info &lt;- get_library_info(user = user)\n\n\nlibrary(stringr)\nlibrary(wordcloud)\n\nLoading required package: RColorBrewer\n\n#wordcloud\n\nartist_info$genres &lt;- str_split(artist_info$artist_tags, \"; \")\ngenres_unlisted &lt;- unlist(artist_info$genres)\n\n# Utworzenie tabeli częstości gatunków\ntabela_czestosci_gatunkow &lt;- table(genres_unlisted)\nlista_nazw_gat &lt;- names(tabela_czestosci_gatunkow)\n\n#chmura słów\npar(mar=c(2,2,2,2))\nwordcloud(lista_nazw_gat, as.numeric(tabela_czestosci_gatunkow), colors = c(\"#101112\", \"#384a8c\", \"#db8904\"))\n\n\n\n\n\n\n\n\n\nnajczestszy_gatunek &lt;- names(tabela_czestosci_gatunkow)[which.max(tabela_czestosci_gatunkow)]\n\ntest_cgest &lt;- rev(sort(tabela_czestosci_gatunkow, deacreasing = TRUE))\n#test_cgest[1]\n\n# Lista najczęstszych gatunków wśród artystów\nfor (i in 1:10){\n  cat(paste0(i, \". \", names(test_cgest[i]), \": \", as.numeric(test_cgest[i]), \"\\n\"))\n}\n\n1. electronic: 345\n2. Hip-Hop: 321\n3. rap: 297\n4. seen live: 279\n5. pop: 224\n6. indie: 203\n7. hip hop: 153\n8. experimental: 114\n9. hyperpop: 111\n10. female vocalists: 110\n\ntc &lt;- head(test_cgest,10)\npar(mar=c(5,2,2,2))\npal &lt;- colorRamp(c(\"#050861\", \"#131ad6\"))\nvalues_barplot &lt;- as.numeric(rev(tc))\nkolor_barplot &lt;- rgb(pal((values_barplot - min(values_barplot))/ diff(range(values_barplot))), max=255)\nbarplot_tracks &lt;- barplot(rev(tc), horiz = T, las = 1, yaxt = \"n\", col = kolor_barplot, xlab = \"Ilość odtworzeń\", main = \"Najczęstsze gatunki muzyczne wśród artystów\")\ntext(as.numeric(rev(tc)), barplot_tracks, labels = as.numeric(rev(tc)), pos = 2, col = \"white\", cex = 0.8, font = 2)\ntext(1, barplot_tracks, labels = paste0(rev(names(tc))), pos = 4, col = \"white\", cex = 0.8, font = 2)"
  },
  {
    "objectID": "Projekty/TS/index.html",
    "href": "Projekty/TS/index.html",
    "title": "2. Time Series forecasting",
    "section": "",
    "text": "Projekt wykonany w języku R w ramach zajęć Applied Macroeconometrics."
  },
  {
    "objectID": "Projekty/TS/index.html#time-series-forecasting",
    "href": "Projekty/TS/index.html#time-series-forecasting",
    "title": "2. Time Series forecasting",
    "section": "Time series forecasting",
    "text": "Time series forecasting\nPodczas mojej wymiany studenckiej w Neuchâtel miałem przyjemność uczestniczyć w zajęciach z makroekonometrii stosowanej, prowadzonych przez prof. Daniela Kaufmana. Na zajęciach uczyliśmy się prognozowania makroekonomicznych szeregów czasowych, a jednym z elementów zaliczenia było przygotowanie prognozy wskaźnika wyznaczonego przez profesora. Moim zadaniem było opracowanie prognozy wartości wskaźnika podaży pieniądza M3 dla Szwajcarii.\nOto mój kod, który zastosowałem do wykonania prognozy.\n\nlibrary(readxl)\nlibrary(xts)\nlibrary(dplyr)\nlibrary(tsbox)\nlibrary(CADFtest)\nlibrary(forecast)\nlibrary(ggplot2)\nlibrary(seasonal)\nlibrary(lmtest)\n\n\nnameMin &lt;- function(Matx){\n  # This is a useful function that returns the column and row names of a matrix\n  # for the minimum value of the matrix\n  \n  ind &lt;- which(Matx == min(Matx), arr.ind = TRUE)\n  cname &lt;- colnames(Matx)[[ind[2]]]\n  rname &lt;- rownames(Matx)[[ind[1]]]\n  return(c(rname, cname))\n  \n}\n\nHistoryczne wartości wskaźnika M3 pobrałem ze strony Szwajcarskiego Banku Centralnego.\n\ndata &lt;- read_excel(\"snb-chart-data-snbmonagglech-en-all-20241021_0900.xlsx\", \n                   skip = 15) # First 15 lines of the file are the information about the data\n\n# Let's inspect the data\n\ndata %&gt;% head(5)\n\n# A tibble: 5 × 9\n  ...1    `Currency in circulation` `Sight deposits` Deposits in transaction a…¹\n  &lt;chr&gt;                       &lt;dbl&gt;            &lt;dbl&gt;                       &lt;dbl&gt;\n1 1984-12                      23.7             44.4                        27.3\n2 1985-01                      22.2             41.6                        28.0\n3 1985-02                      22.2             38.6                        28.4\n4 1985-03                      22.3             39.6                        28.3\n5 1985-04                      22.0             40.0                        28.4\n# ℹ abbreviated name: ¹​`Deposits in transaction accounts`\n# ℹ 5 more variables: `Savings deposits` &lt;dbl&gt;, `Time deposits` &lt;dbl&gt;,\n#   M1 &lt;dbl&gt;, M2 &lt;dbl&gt;, M3 &lt;dbl&gt;\n\n\n\nStacjonarność szeregu\nZobaczymy jak dane prezentują się na wykresie.\n\n# Modifying the date vector to convert it later to a POSIX format\nM3 &lt;- xts(x = data$M3,\n          order.by= as.Date(sapply(data[,1], paste0, \"-01\"))\n)\n\nts_plot(M3, \n        title = \"Swiss Monetary aggregate M3 over time\",\n        subtitle = \"in billions of CHF\",\n        ylab = \"CHF billions\")\n\n\n\n\n\n\n\n\nWizualnie można zauważyć, że poziom M3 nie jest stacjonarny i wykazuje trend rosnący.\nSprawdźmy to również matematycznie.\nDo tego użyję testu Augmented Dickey-Fuller (ADF), który służy do sprawdzania, czy szereg czasowy jest stacjonarny.\n\nunitRootTest &lt;- CADFtest(M3, type = \"trend\", max.lag.y=10)\nsummary(unitRootTest)\n\nAugmented DF test \n                                             ADF test\nt-test statistic:                          -1.4983573\np-value:                                    0.8291424\nMax lag of the diff. dependent variable:   10.0000000\n\nCall:\ndynlm(formula = formula(model), start = obs.1, end = obs.T)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-14.6944  -2.3640   0.2272   2.3451  15.3185 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.522721   0.614911   2.476 0.013638 *  \ntrnd         0.010613   0.006340   1.674 0.094817 .  \nL(y, 1)     -0.004497   0.003001  -1.498 0.829142    \nL(d(y), 1)   0.109869   0.046814   2.347 0.019357 *  \nL(d(y), 2)  -0.009863   0.046899  -0.210 0.833526    \nL(d(y), 3)   0.215642   0.046840   4.604 5.39e-06 ***\nL(d(y), 4)   0.042155   0.047272   0.892 0.373000    \nL(d(y), 5)  -0.049811   0.047393  -1.051 0.293816    \nL(d(y), 6)   0.097265   0.047411   2.052 0.040790 *  \nL(d(y), 7)  -0.174298   0.047666  -3.657 0.000285 ***\nL(d(y), 8)   0.041753   0.047302   0.883 0.377876    \nL(d(y), 9)   0.104568   0.047378   2.207 0.027807 *  \nL(d(y), 10)  0.030430   0.047362   0.642 0.520873    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.908 on 454 degrees of freedom\nMultiple R-squared:  0.126, Adjusted R-squared:  0.1029 \nF-statistic: 6.088 on 10 and 454 DF,  p-value: 1.016e-08\n\n\nHipoteza zerowa testu Dickey-Fuller mówi o tym, że szereg czasowy jest niestacjonarny. W teście p-value wynosi około 0,8 a to oznacza że nie ma podstaw do odrzucenia hipotezy zerowej.\nAby usunąć trend i przekształcić szereg w stacjonarny, muszę zróżnicować dane. Robię to obliczając procentowe zmiany szeregu czasowego za pomocą funkcji ts_pc().\n\nts_plot(ts_pc(M3), \n        title = \"Swiss Monetary aggregate M3 over time\",\n        subtitle = \"month to month percentage change\",\n        ylab = \"%\")\n\n\n\n\n\n\n\n\nWizualnie nie widzę większego trendu, ale sprawdźmy to za pomocą testu ADF.\n\nunitRootTest &lt;- CADFtest(ts_pc(M3), type = \"drift\", max.lag.y=10)\nsummary(unitRootTest)\n\nAugmented DF test \n                                                ADF test\nt-test statistic:                          -5.1349687627\np-value:                                    0.0000140977\nMax lag of the diff. dependent variable:   10.0000000000\n\nCall:\ndynlm(formula = formula(model), start = obs.1, end = obs.T)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.31049 -0.40007  0.00915  0.33484  1.86934 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.19675    0.04853   4.054 5.92e-05 ***\nL(y, 1)     -0.61773    0.12030  -5.135 1.41e-05 ***\nL(d(y), 1)  -0.22043    0.11685  -1.886  0.05987 .  \nL(d(y), 2)  -0.25948    0.11201  -2.317  0.02097 *  \nL(d(y), 3)  -0.08458    0.10568  -0.800  0.42393    \nL(d(y), 4)  -0.10319    0.09877  -1.045  0.29669    \nL(d(y), 5)  -0.17109    0.09360  -1.828  0.06823 .  \nL(d(y), 6)  -0.05818    0.08609  -0.676  0.49950    \nL(d(y), 7)  -0.19022    0.07908  -2.405  0.01655 *  \nL(d(y), 8)  -0.20980    0.07231  -2.902  0.00389 ** \nL(d(y), 9)  -0.12590    0.06040  -2.084  0.03768 *  \nL(d(y), 10) -0.10073    0.04639  -2.172  0.03040 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6302 on 454 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  0.4839,    Adjusted R-squared:  0.4714 \nF-statistic: 4.536 on 10 and 454 DF,  p-value: 3.868e-06\n\n\nP-value jest bardzo małe, szereg jest stacjonarny wokół stałej średniej. To na nim mogę teraz wykonać analizę. Przy interpretacji wyników trzeba pamiętać o powrotnej transformacji szeregu do oryginalnej formy.\nTeraz sprawdźmy czy występuje sezonowość. Jeśli tak to trzeba będzie ją usunąć.\n\nacf(as.numeric(na.omit(ts_pc(M3))), lag.max = 24, plot = T)\n\n\n\n\n\n\n\n\nTak, sezonowość występuje. Widzimy to po znaczącej autokorelacji na dwunastym i dwudziestym czwartym opóźnieniu.\nAby usunąć sezonowość, zastosuję bibliotekę seasonal.\nZobaczymy jak wygląda wykres wskaźnika po usunięciu sezonowości.\n\nM3seas &lt;- final(seas(ts_ts(M3)))\n\n# The data is not stationary\nts_plot(M3seas, \n        title = \"Swiss Monetary aggregate M3 over time\",\n        subtitle = \"seasonally adjusted, in billions of CHF\",\n        ylab = \"CHF billions\")\n\n\n\n\n\n\n\n\nSprawdźmy czy na pewno szereg czasowy zróżnicowany zmianami procentowymi z miesiąca na miesiąc nadal jest stacjonarny.\n\nM3seas_pc &lt;- ts_pc(M3seas)\nts_plot(M3seas_pc, \n        title = \"Swiss Monetary aggregate M3 over time\",\n        subtitle = \"Seasonly adjusted, month to month percentage change\",\n        ylab = \"%\")\n\n\n\n\n\n\n\n\n\n# The month on month growth rate is stationary \n\nunitRootTest &lt;- CADFtest(M3seas_pc, type = \"drift\", max.lag.y=10)\nsummary(unitRootTest)\n\nAugmented DF test \n                                                ADF test\nt-test statistic:                          -4.1864117563\np-value:                                    0.0007741003\nMax lag of the diff. dependent variable:   10.0000000000\n\nCall:\ndynlm(formula = formula(model), start = obs.1, end = obs.T)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.38265 -0.26569 -0.03447  0.25565  1.45380 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.12018    0.03632   3.309 0.001012 ** \nL(y, 1)     -0.37936    0.09062  -4.186 0.000774 ***\nL(d(y), 1)  -0.52249    0.09366  -5.578 4.18e-08 ***\nL(d(y), 2)  -0.51053    0.09435  -5.411 1.02e-07 ***\nL(d(y), 3)  -0.34446    0.09478  -3.634 0.000311 ***\nL(d(y), 4)  -0.21890    0.09283  -2.358 0.018794 *  \nL(d(y), 5)  -0.16850    0.09058  -1.860 0.063479 .  \nL(d(y), 6)  -0.09920    0.08707  -1.139 0.255209    \nL(d(y), 7)  -0.16145    0.08254  -1.956 0.051075 .  \nL(d(y), 8)  -0.09718    0.07541  -1.289 0.198122    \nL(d(y), 9)  -0.09096    0.06300  -1.444 0.149500    \nL(d(y), 10)  0.01368    0.04691   0.292 0.770646    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4687 on 454 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  0.4696,    Adjusted R-squared:  0.4568 \nF-statistic:  4.81 on 10 and 454 DF,  p-value: 1.364e-06\n\n\nSzereg czasowy po usunięciu sezonowości nadal jest stacjonarny.\n\n\nPrognozowanie\nTeraz możemy przejść do prognozowania. Do tego użyje modelu ARMA.\nModele ARMA to modele szeregów czasowych łączące składnik autoregresyjny (AR), który opisuje zależność wartości od jej wcześniejszych obserwacji, oraz składnik średniej ruchomej (MA), uwzględniający wpływ losowych szoków z przeszłości, co pozwala na modelowanie i prognozowanie stacjonarnych szeregów czasowych.\nPytaniem jest jaka kombinacja składników autoregresyjnych oraz składników średniej ruchomej jest optymalna w modelu. W tym celu, jako że dane są na małą skalę, zbuduje pewną kombinacje możliwych modeli, i porównam ich wydajność za pomocą kryteriów informacji AIC i BIC.\n\nmaxP = 6   # Maksymalna liczba opóźnień AR\nmaxQ = 6   # Maksymalna liczba opóźnień MA\n\n# Obiekty do przechowywania kryteriów dla każdej możliwej struktury opóźnień\nAIC = matrix(data=NA, nrow=maxP+1, ncol=maxQ+1)\nBIC = matrix(data=NA, nrow=maxP+1, ncol=maxQ+1)\ncolnames(AIC) = 0:maxQ\ncolnames(BIC) = 0:maxQ\nrownames(AIC) = 0:maxP\nrownames(BIC) = 0:maxP\n\nfor (p in 0:maxP) {\n  for (q in 0:maxQ) {\n    \n    # Estymacja odpowiadającego modelu\n    temp = Arima(M3seas_pc, order = c(p, 0, q), include.constant= TRUE)\n    \n    # Zapisanie wartości kryteriów informacyjnych\n    AIC[p+1, q+1] = temp$aic\n    BIC[p+1, q+1] = temp$bic\n  }\n}\n\n# Znalezienie rzędu opóźnień o najmniejszej wartości kryterium informacyjnego \n# (nameMin() to funkcja użytkownika, która pozwala znaleźć nazwę kolumn i wierszy \n# odpowiadających minimalnej wartości w macierzy)\nminCritAIC = nameMin(AIC)\nminCritBIC = nameMin(BIC)\n\nprint(\"Optymalny rząd opóźnień według AIC i BIC (p, q)\")\n\n[1] \"Optymalny rząd opóźnień według AIC i BIC (p, q)\"\n\nminCritAIC # 5 5\n\n[1] \"5\" \"5\"\n\nminCritBIC # 1 1\n\n[1] \"1\" \"1\"\n\n\nNajlepsze modele to 1 1 oraz 5 5. Porównajmy je teraz.\n\n# ARMA(1,1)\nModel &lt;- Arima(M3seas_pc, order = c(1, 0, 1), include.constant= TRUE)\nsum &lt;- summary(Model)\nsum\n\nSeries: M3seas_pc \nARIMA(1,0,1) with non-zero mean \n\nCoefficients:\n         ar1      ma1    mean\n      0.9444  -0.8404  0.3208\ns.e.  0.0270   0.0431  0.0600\n\nsigma^2 = 0.2198:  log likelihood = -314.18\nAIC=636.37   AICc=636.45   BIC=653.04\n\nTraining set error measures:\n                       ME      RMSE       MAE      MPE     MAPE      MASE\nTraining set -0.001517333 0.4673958 0.3575627 58.35058 199.3537 0.6488185\n                    ACF1\nTraining set -0.01533273\n\n\n\ncheckresiduals(Model) # The lag 12 months is significant\n\n\n\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(1,0,1) with non-zero mean\nQ* = 41.554, df = 22, p-value = 0.007075\n\nModel df: 2.   Total lags used: 24\n\n\nZauważam że Autoregresja 12 rzędu ma znaczącą autokorelację. Rozkład reszt odbiega też od postaci normalnej. Sprawdźmy czy w resztach modelu występuje autokorelacja. Robię to z pomocą testu Ljung-Box. Hipoteza zerowa stanowi, że wśród reszt nie ma autokorelacji.\n\nbox &lt;- Box.test(Model$residuals, lag=24, fitdf=1, type=\"Lj\")\nbox # P value is 0.01\n\n\n    Box-Ljung test\n\ndata:  Model$residuals\nX-squared = 41.554, df = 23, p-value = 0.01023\n\n\nP value wynosi 0,01, więc odrzucamy hipotezę zerową. Wśród reszty występuje autokorelacja.\nSprawdźmy jak wypada drugi model.\n\n# ARMA (5,5)\nModel5_5 &lt;- Arima(M3seas_pc, order = c(5, 0, 5), include.constant= TRUE)\nsum &lt;- summary(Model5_5)\nsum\n\nSeries: M3seas_pc \nARIMA(5,0,5) with non-zero mean \n\nCoefficients:\n         ar1     ar2      ar3      ar4     ar5      ma1      ma2     ma3\n      0.1861  0.3803  -0.3290  -0.2260  0.7795  -0.0859  -0.3415  0.4768\ns.e.  0.0555  0.0450   0.0424   0.0418  0.0388   0.0594   0.0430  0.0267\n         ma4      ma5   mean\n      0.3836  -0.8179  0.320\ns.e.  0.0393   0.0523  0.059\n\nsigma^2 = 0.2053:  log likelihood = -296.88\nAIC=617.77   AICc=618.44   BIC=667.78\n\nTraining set error measures:\n                       ME      RMSE       MAE      MPE     MAPE      MASE\nTraining set -0.001012154 0.4478029 0.3407682 44.87703 178.0506 0.6183438\n                    ACF1\nTraining set 0.002370316\n\n\n\ncheckresiduals(Model5_5) # There is no significance on the 12th leg\n\n\n\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(5,0,5) with non-zero mean\nQ* = 20.226, df = 14, p-value = 0.1232\n\nModel df: 10.   Total lags used: 24\n\n\nO ile na jednym z opóźnień występuje istotna statystycznie autokorelacja, to rozkład reszt jest zdecydowanie bliższy rozkładowi normalnemu niż wcześniej.\n\nbox &lt;- Box.test(Model5_5$residuals, lag=24, fitdf=1, type=\"Lj\")\nbox \n\n\n    Box-Ljung test\n\ndata:  Model5_5$residuals\nX-squared = 20.226, df = 23, p-value = 0.6282\n\n\nI rzeczywiście, nie odrzucamy hipotezy zerowej, co świadczy o braku autokorelacji reszt modelu. Tak więc do dalszej prognozy użyję modelu ARMA(5,5).\nI to ten model wrzucam dalej do funkcji forecast.\n\nhorizon = 28 # to Jan 1 2027\nForecast = forecast(Model5_5, h = horizon, level = c(50,80,95))\nautoplot(Forecast)+theme_minimal()\n\n\n\n\n\n\n\n\nRezultat to przewidywane miesięczne zmiany procentowe dla przyszłych 28 obserwacji.\nTeraz przeprowadzam transformację z powrotem do wartości nominalnych.\n\n# Concatenating the historical data with the forecast\ntemp1 = ts_bind(Forecast$x, Forecast$mean)\n\n# I want to keep the original structire of the time series but remove the data\ntemp2=temp1\ntemp2[]=NA\n\n# Let the first value be the the first value from historical data\ntemp2[1] = as.numeric(M3seas[1])\n\n# Let every next value be defined as:\n# value for period t = value for period t-1 * (1 + percentage growth from t-1 to t)\nfor (observation in 2:length(temp1)){\n  temp2[observation] &lt;- temp2[observation-1] * (1+temp1[observation]/100)\n}\n\n# We have a plot for the month on month growth rate, but I would prefer to show the forecast in absolute values\nfcstStart = ts_summary(Forecast$mean)$start\nhistEnd   = ts_summary(Forecast$x)$end\nHistYoY   = ts_span(temp2, NULL, histEnd)\nFcstYoY   = ts_span(temp2, histEnd, NULL)\n\nTutaj wizualizuję swoją prognozę. Moim celem jest również zaznaczenie przedziałów ufności na wykresie.\n\n# User defined function that takes different vectors of predicted month-on-month growth, and converts them \n# to absolute values using the historical M3 data\n# In the code it's used for drawing the different confidence intervals\n\nts_forecast_plot &lt;- function(data){\n  # First value of the drawing is the last value from historical data, so wo dont have a gap with the drawiwing\n  templower = ts_span(temp1, histEnd, NULL)\n  templower[] = NA\n  #Using the formula from earlier\n  templower[1] = M3seas[length(M3seas)]\n  for (observation in 2:length(templower)){\n    templower[observation] &lt;- templower[observation-1] * (1+data[observation]/100)\n  }\n  templower # function returns times series of absolute values for the forecast\n}\n\nplot_index &lt;- index(FcstYoY)\n\n#Plotting the forecast\nplot(index(HistYoY), HistYoY, type = \"l\", xlim = c(2019, 2026), ylim = c(1000, 1400), \n     ylab = \"Billions of CHF\", xlab = \"Year\", \n     main = \"Swiss M3 money supply forecast\")\n\npolygon(c(plot_index[-29], rev(plot_index[-29])), c(ts_forecast_plot(Forecast$lower[,3])[-29], rev(ts_forecast_plot(Forecast$upper[,3])[-29])),\n        col = rgb(0,0,1,.1), lty = 0) # 95%\npolygon(c(plot_index[-29], rev(plot_index[-29])), c(ts_forecast_plot(Forecast$lower[,2])[-29], rev(ts_forecast_plot(Forecast$upper[,2])[-29])),\n        col = rgb(0,0,1,.4), lty = 0) # 80%\npolygon(c(plot_index[-29], rev(plot_index[-29])), c(ts_forecast_plot(Forecast$lower[,1])[-29], rev(ts_forecast_plot(Forecast$upper[,1])[-29])),\n        col = rgb(0,0,1,.7), lty = 0) # 50%\nlegend(2019, 1400, legend=c(\"95% confidence\", \"80% confidence\", \"50% confidence\", \"Forecast mean\"),\n       col=c(rgb(0,0,1,.1), rgb(0,0,1,.4), rgb(0,0,1,.7), \"black\"), lty = 1, lwd = c(12,12,12,2))\nlines(index(FcstYoY), FcstYoY, type = \"l\", col = \"black\", lwd = 2)"
  },
  {
    "objectID": "Projekty/MLstocks/index.html",
    "href": "Projekty/MLstocks/index.html",
    "title": "8. Forecasting stocks with ML",
    "section": "",
    "text": "Projekt wykonany w Pythonie na przedmiot Programming."
  },
  {
    "objectID": "Projekty/R6Class/index.html",
    "href": "Projekty/R6Class/index.html",
    "title": "7. R6Class",
    "section": "",
    "text": "Projekt wykonany w ramach zajęć Zaawansowane programowanie w języku R.\n\nOpis projektu\nProjekt dotyczy rozszerzonej reprezentacji szeregu czasowego. W języku R jest wiele możliwości reprezentowania szeregów czasowych. Podstawowe klasy to ts i mts. Klasy, które rozszerzają te klasy to zoo i xts. Wszystkie te klasy reprezentują jedynie dane a więc dane dotyczące czasu i wartości. W tym projekcie chcemy do danych dołożyć operacje, które są wykonywane na szeregach czasowych oraz metody, które tworzą prognozy.\nTechnicznie, zadaniem w projekcie jest zdefiniowanie klasy timeSeries w modelu obiektowym R6, która na podstawie wektora określającego czas i wektora wartości, inicjalizuje instancję klasy. Metoda new powinna przyjmować dwa argumenty: times, który przyjmuje wektor czasów (i tutaj możemy wykorzystać dowolną klasę reprezentującą datę i czas, np. Date czy yearmon) oraz values, który przyjmuje wektor wartości.\nKlasa musi oferować następujące podstawowe metody.\n\nMetoda getTimes, która zwraca podany wektor czasów.\nMetoda getValues, która zwraca podany wektor wartości.\nMetoda getTimeSeries, która zwraca szereg czasowy. W rozwiązaniu proszę wykorzystać klasę xts.\n\nProjekt jest zatem rozszerzeniem klasy xts.\nPowyższe metody są typowymi fundamentalnymi metodami związanymi z szeregiem czasowym.\n\n\nMoja definicja funkcji\n\nlibrary(R6)\nlibrary(xts)\nlibrary(zoo)\nlibrary(tibble)\nlibrary(ggplot2)\n\ntimeSeries &lt;- R6Class(\n  classname = \"timeSeries\",\n  public = list(\n    times = NA,\n    values = NA,\n    table = NULL,\n    operations = list(),\n    models = list(),\n    paths = list(),\n    initialize = function(times, values) {\n      self$times &lt;- times\n      self$values &lt;- values\n      self$table &lt;- xts::xts(x = self$values, order.by = self$times)\n    },\n    \n    getTimes = function() {\n      self$times\n    },\n    \n    getValues = function() {\n      self$values\n    },\n    \n    getTimeSeries = function() {\n      self$table\n    },\n    \n    opsAppend = function(...) {\n      ops &lt;- list(...)\n      for (name in names(ops)) {\n        self$operations[[name]] &lt;- ops[[name]]\n      }\n    },\n    \n    opsRemove = function(...) {\n      keys &lt;- list(...)\n      for (key in keys) {\n        self$operations[[key]] &lt;- NULL\n      }\n    },\n    \n    opsList = function() {\n      data.frame(id = seq_along(self$operations), name = names(self$operations))\n    },\n    modelsAppend = function(...){\n      model &lt;- list(...)\n      for (name in names(model)) {\n        self$models[[name]] &lt;- model[[name]]\n      }\n    },\n    modelsRemove = function(...) {\n      keys &lt;- list(...)\n      for (key in keys) {\n        self$models[[key]] &lt;- NULL\n      }\n    },\n    modelsList = function() {\n      data.frame(id = seq_along(self$models), name = names(self$models))\n    },\n    pathsAppend = function(pathName, operations, model){\n      self$paths[[pathName]] &lt;- list(operations = operations, model = model)\n      },\n    pathsRemove = function(...) {\n      pathNames &lt;- list(...)\n      for (pathName in pathNames) {\n        self$paths[[pathName]] &lt;- NULL\n      }\n    },\n    pathsList = function() {\n      paths_df &lt;- do.call(rbind, lapply(names(self$paths), function(name) {\n        data.frame(\n          id = which(names(self$paths) == name),\n          name = name,\n          operations = paste(self$paths[[name]]$operations, collapse = \" → \"),\n          model = self$paths[[name]]$model,\n          stringsAsFactors = FALSE\n        )\n      }))\n      paths_df\n    },\n    pathsRun = function(pathName) {\n      if (!(pathName %in% names(self$paths))) {\n        stop(\"Path not found.\")\n      }\n      path &lt;- self$paths[[pathName]]\n      result &lt;- self$table\n      for (op in path$operations) {\n        result &lt;- self$operations[[op]](result)\n      }\n      prediction &lt;- self$models[[path$model]](result)\n      prediction\n    }\n    \n  )\n)\n\n\n\nPrzykłady\n\nPrzykład 1\nPoniższy przykład tworzy bardzo prosty szereg czasowy i pokazuje działanie opisanych powyżej metod.\nPrzykład pokazuje inicjalizację obiektu klasy timeSeries oraz wykorzytanie podstawowych metod. Na tej podstawie tworzony jest prosty wykres szeregu czasowego.\n\n### Tworzenie danych do szeregu czasowego: \nts &lt;- yearmon(2009) + (0:(5 * 12 - 1)) / 12\nvs &lt;- seq_along(ts) / 5 + cumsum(rnorm(5 * 12))\n\n### Inicjalizacja obiektu\ny &lt;- timeSeries$new(times = ts, values = vs)\n\n### Pobranie wartości\ny$getValues()\n\n [1]  2.02668578  1.69543891  1.43767406  2.92703291  1.90834614  2.73060705\n [7]  3.35871311  3.25160534  2.80952381  1.89143238  4.53993968  4.12637763\n[13]  2.93070107  0.69957067  0.30442847 -0.26986936 -1.35025937 -1.72544354\n[19] -1.31934086 -2.53161781 -2.17209745 -0.63832369 -0.34461294 -1.13688044\n[25] -2.58551089 -1.24550950 -2.03329902 -2.54733605 -2.39231802 -2.45211124\n[31] -1.32883929 -1.21971984 -2.47686809 -1.49599398 -2.12757172 -0.52417055\n[37] -0.28555002 -0.08249298 -0.14911944 -2.12008618 -2.76265308 -4.69377435\n[43] -3.46503138 -2.41113913 -2.61544238 -1.95864620 -2.10922700 -4.56082348\n[49] -3.84402464 -4.13551034 -3.04930291 -1.46853961 -1.68439819 -1.06127529\n[55] -2.48557023 -3.49004290 -2.53821511 -1.45470203 -1.35969134 -1.05172806\n\n### Pobranie wektora czasów\ny$getTimes()\n\n [1] \"Jan 2009\" \"Feb 2009\" \"Mar 2009\" \"Apr 2009\" \"May 2009\" \"Jun 2009\"\n [7] \"Jul 2009\" \"Aug 2009\" \"Sep 2009\" \"Oct 2009\" \"Nov 2009\" \"Dec 2009\"\n[13] \"Jan 2010\" \"Feb 2010\" \"Mar 2010\" \"Apr 2010\" \"May 2010\" \"Jun 2010\"\n[19] \"Jul 2010\" \"Aug 2010\" \"Sep 2010\" \"Oct 2010\" \"Nov 2010\" \"Dec 2010\"\n[25] \"Jan 2011\" \"Feb 2011\" \"Mar 2011\" \"Apr 2011\" \"May 2011\" \"Jun 2011\"\n[31] \"Jul 2011\" \"Aug 2011\" \"Sep 2011\" \"Oct 2011\" \"Nov 2011\" \"Dec 2011\"\n[37] \"Jan 2012\" \"Feb 2012\" \"Mar 2012\" \"Apr 2012\" \"May 2012\" \"Jun 2012\"\n[43] \"Jul 2012\" \"Aug 2012\" \"Sep 2012\" \"Oct 2012\" \"Nov 2012\" \"Dec 2012\"\n[49] \"Jan 2013\" \"Feb 2013\" \"Mar 2013\" \"Apr 2013\" \"May 2013\" \"Jun 2013\"\n[55] \"Jul 2013\" \"Aug 2013\" \"Sep 2013\" \"Oct 2013\" \"Nov 2013\" \"Dec 2013\"\n\n### Pobranie szeregu czasowgo (klasa xts)\nhead(y$getTimeSeries(),10)\n\n             [,1]\nJan 2009 2.026686\nFeb 2009 1.695439\nMar 2009 1.437674\nApr 2009 2.927033\nMay 2009 1.908346\nJun 2009 2.730607\nJul 2009 3.358713\nAug 2009 3.251605\nSep 2009 2.809524\nOct 2009 1.891432\n\n### Stworzenie przykładowego wykresu\nggplot(\n  data = tibble(time = y$getTimes(), value = y$getValues()),\n  mapping = aes(x = time, y = value)\n) +\n  geom_line() +\n  geom_point() +\n  labs(title = \"Example of a time series\") +\n  theme_light()\n\n\n\n\n\n\n\n\n\n\nPrzykład 2\nPodstawowe metody, opisane powyżej, nie wyczerpują operacji, które chcemy dodać do szeregu czasowego. Oczywiście katalog potencjalnych operacji nie jest skończony więc nie możemy dodać metod reprezentujących wszystkie potencjalne operacje. Z tego powodu chcemy mieć możliwość dodawania dowolnych operacji i następnie wiązania tych operacji. W pierwszej kolejności chcemy zbudować prosty system zarządzania operacjami. Na potrzeby tego projektu, operacja na szeregu czasowym, to dowolna funkcja, która jako argument przyjmuje szereg czasowy (klasa xts) i zwraca szereg czasowy (ponownie klasa xts). Tak więc musimy mieć następujące metody.\n\nMetoda opsAppend do dodawania operacji. Składania tej metody powinna być identyczna ze składnią funkcji list. Konieczne jest podanie kluczy jednoznacznie identyfikujących\nMetoda opsRemove do usuwania operacji. Metoda przyjmuje dowolną liczbę stringów, które są kluczami usuwanych operacji.\nMetoda opsList, która listuje operacje, które aktualnie są w liście dodanych operacji.\n\nPoniższy przykład pokazuje metody dodawania, listowania i usuwania operacji.\nW przykładzie tworzymy nowy obiekt, następnie dodajemy, listujemy i usuwamy operacje.\n\n### Tworzenie danych do szeregu czasowego: \nts &lt;- yearmon(2009) + (0:(5 * 12 - 1)) / 12\nvs &lt;- seq_along(ts) / 5 + cumsum(rnorm(5 * 12))\n\n### Inicjalizacja obiektu\ny &lt;- timeSeries$new(times = ts, values = vs)\n\n### Dodawanie operacji na szeregu czasowycvh\ny$opsAppend(\n  differencing = function(x) { diff(x = x, lag = 1, differences = 1) },\n  logs_abs = function(x) { log(abs(x)) },\n  na_omit = na.omit\n)\n\n### Listowanie operacji\ny$opsList()\n\n  id         name\n1  1 differencing\n2  2     logs_abs\n3  3      na_omit\n\n### Usuwanie operacji\ny$opsRemove(\"logs_abs\")\n\n### Listowanie operacji\ny$opsList()\n\n  id         name\n1  1 differencing\n2  2      na_omit\n\n\n\n\nPrzykład 3\nKolejnym elementem, które musi się znaleźć w klasie jest model predykcji. Na potrzeby tego projektu, przez model predkcji będziemy rozumieli dowolną funkcję, która jako argument przyjmuje obiekt klasy xts i zwraca obiekt tej klasy. Ponownie, katalog możliwych modeli predykcji nie jest zamknięty, więc również musimy mieć prosty system zarządzania. W szczególności klasa musi oferować następujące metody.\n\nMetoda modelsAppend, która pozwala na dodawanie modeli. Składnia tej metody powinna być taka jak funkcji list. Podobnie jak poporzednio, konieczne jest podanie unikalnych kluczy indetyfikujących modele.\nMetoda modelsRemove, która pozwana na usunięcie modeli. Metoda musi przyjmować stringi, które definiują dodane modele.\nMetoda modelsList, która listuje dodane modele.\n\nPoniższy przykład pokazuje zastosowanie opisanych metod.\nW przykładzie tworzymy nowy obiekt, następnie dodajemy, listujemy i usuwamy modele.\n\n### Tworzenie danych do szeregu czasowego: \nts &lt;- yearmon(2009) + (0:(5 * 12 - 1)) / 12\nvs &lt;- seq_along(ts) / 5 + cumsum(rnorm(5 * 12))\n\n### Inicjalizacja obiektu\ny &lt;- timeSeries$new(times = ts, values = vs)\n\n### Dodawanie prostego modelu liniowego\ny$modelsAppend(\n  linear_prediciton = function(x) {\n    dtemp &lt;- data.frame(\n      t = seq_along(coredata(x)),\n      z = coredata(x)\n    )\n    m &lt;- lm(formula = z ~ t, data = dtemp)\n    p &lt;- predict(\n      object = m,\n      newdata = data.frame(t = last(dtemp$t) + 1:6)\n    )\n    xts(\n      x = p,\n      order.by = index(last(x)) + (1:6) * deltat(x)\n    )\n  }\n)\n\n## Dodawanie modelu identyczności\ny$modelsAppend(\n  identity_prediction = function(x) {\n    x\n  }\n)\n\n### Listowanie modelu\ny$modelsList()\n\n  id                name\n1  1   linear_prediciton\n2  2 identity_prediction\n\n### Usuwanie modelu\ny$modelsRemove(\"identity_prediction\")\n\n### Listowanie modelu\ny$modelsList()\n\n  id              name\n1  1 linear_prediciton\n\n\n\n\nPrzykład 4\nSame operacje oraz modele nic nie robią. Kolejne zadanie to wiązanie operatorów i kończenie ich modelami predykcji. Powiązanie takie ma postać szereg czasowy → operator 1 → operator 2 → … → operator n → model. Każde takie powiązanie nazywamy dalej ścieżką obliczeniową (path). Klasa musi mieć narzędzia do tworzenia i zarządzania takimi ścieżkami obliczeniowymi. Klasa musi oferować następujące metody.\n\nMetoda pathsAppend musi pozwalać dodawać ścieżki obliczeniowe. Metoda ta powinna przyjmować trzy argumenty. Argument pathName, który jest stringiem i jest unikalny. Argument operations, który jest wektorem stringów definiujących dodane operacje. Argument model, który jest stringiem definiującym model predykcji. Razem metoda pozwala definiować ścieżkę obliczeniową.\nMetoda pathsRemove musi pozwalać na usuwanie poprzednio zdefiniowanych ścieżek obliczeniowych. Metoda przyjmuje dowolną liczbę stringów definiujących ścieżki obliczeniowe.\nMetoda pathsList, która listuje zdefiniowane ścieżki.\nMetoda pathsRun, która przyjmuje argument pathName, który jest stringiem definiującym ścieżkę.\n\nMetoda ta wykonuje obliczenia dla zdefiniowanej ścieżki a więc w kolejności zdefiniowanej w ścieżce stosuje funkcje definiujące operacje i na końcu model obliczeniowy. Wynikiem jest więc prognoza dla szeregu czasowego.\n\nlibrary(dplyr)\ndetach(\"package:dplyr\", unload = TRUE)\n\n### Tworzenie danych do szeregu czasowego:\nts &lt;- yearmon(2009) + (0:(2 * 12 - 1)) / 12\nvs &lt;- seq_along(ts) / 10 + cumsum(rnorm(2 * 12))\n\n### Inicjalizacja obiektu\ny &lt;- timeSeries$new(times = ts, values = vs)\n\n### Dodawanie operacji na szeregu czasowycvh\ny$opsAppend(\n  differencing = function(x) { diff(x = x, lag = 1, differences = 1) },\n  logs_abs = function(x) { log(abs(x)) },\n  na_omit = na.omit\n)\n\n### Dodawanie prostego modeli liniowego\ny$modelsAppend(\n  linear_prediciton = function(x) {\n    dtemp &lt;- data.frame(\n      t = seq_along(coredata(x)),\n      z = coredata(x)\n    )\n    m &lt;- lm(formula = z ~ t, data = dtemp)\n    p &lt;- predict(\n      object = m,\n      newdata = data.frame(t = last(dtemp$t) + 1:6)\n    )\n    xts(\n      x = p,\n      order.by = index(last(x)) + (1:6) * deltat(x)\n    )\n  }\n)\n\n## Dodawanie modelu identycznościowego\ny$modelsAppend(\n  identity_prediction = function(x) {\n    x\n  }\n)\n\n### Listing operacji i modeli\ny$opsList()\n\n  id         name\n1  1 differencing\n2  2     logs_abs\n3  3      na_omit\n\ny$modelsList()\n\n  id                name\n1  1   linear_prediciton\n2  2 identity_prediction\n\n### Dodawanie ścieżki z modelem liniowym\ny$pathsAppend(\n  pathName = \"linear with differencing\",\n  operations = c(\"differencing\", \"na_omit\"),\n  model = \"linear_prediciton\"\n)\n\n### Dodawanie ścieżki z modelem identycznościowym / pozwala na\n### zobaczenia co dokładnie wchodzi do modelu predykcji\ny$pathsAppend(\n  pathName = \"identity with differencing\",\n  operations = c(\"differencing\", \"na_omit\"),\n  model = \"identity_prediction\"\n)\n\n### Listowanie zdefiniowanych ścieżek\n\ny$pathsList()\n\n  id                       name             operations               model\n1  1   linear with differencing differencing → na_omit   linear_prediciton\n2  2 identity with differencing differencing → na_omit identity_prediction\n\n### Obliczanie zdefiniowanych ścieżek\nz &lt;- y$pathsRun(pathName = \"identity with differencing\")\nzF &lt;- y$pathsRun(pathName = \"linear with differencing\")\n\n### Tworzenie wykresu na podstawie wyników\nggplot(\n  data = tibble(time = index(z), value = coredata(z)),\n  mapping = aes(x = time, y = value)\n) +\n  geom_line() +\n  geom_point() +\n  geom_line(data = tibble(time = index(zF), value = coredata(zF)), color = \"red\") +\n  geom_point(data = tibble(time = index(zF), value = coredata(zF)), color = \"red\") +\n  coord_cartesian(ylim = c(-5, 5)) +\n  labs(title = \"Time series and forecast\") +\n  theme_light()"
  },
  {
    "objectID": "Projekty/MLR/index.html",
    "href": "Projekty/MLR/index.html",
    "title": "5. Function Factory",
    "section": "",
    "text": "Projekt wykonany w ramach zajęć Zaawansowane programowanie w języku R.\n\n\nOpis projektu\nZadanie polega na napisaniu funkcji createFitter(), która przyjmuje tylko jeden argument size. Zadaniem funkcji jest stworzenie nowej funkcji, tzw. fittera, która zawiera prostą sięć neuronową MLP o jednej warstwie ukrytej o wielkości size. Funkcja ta powinna przyjmować dwa argumenty: data, który jest ramką danych (alternatywnie obiektem dziedziczącym z data.frame, np. tbl_df) oraz formula, który jest formuła opisującą co w podanej ramce danych jest targetem a co features. Przykładowo, jeżeli w ramace danych mamy zmienne (kolumny) x i y to formuła y ~ x oznacza, że sieć na podstawie zmiennej x prognozuje zmienną y. Oczywiście powinny być wspierane skróty, np. y ~ . oznacza, że zmienna y jest prognozowana na podstawie wszystkich pozostałych w ramce danych zmiennych. Zadaniem fittera jest zdefiniowane nowej kolejnej funkcji, która jest predyktorem a więc funkcją, która dla podanych zmiennych zwraca predykcje. Tak więc fitter na podstawie podanej próby trenuje sieć neuronową i zwraca funkcję, predyktor, która zawiera wytrenowaną sieć neuronową i jej zadaniem jest tworzenie predykcji z wykorzystaniem wytrenowanej sieci neuronowej.\n\n\nMoje rozwiązanie\n\nlibrary(dplyr)\n\ncreateFitter &lt;- function(size) {\n  #createFitter upewnia się, że mamy właściwe biblioteki oraz tworzy kolejną funkcję\n  if(!require(nnet)){\n    install.packages(\"nnet\")\n    library(nnet)\n  } else {\n    library(nnet)\n  }\n  # Funkcja createFitter zwraca kolejną funckję \"fitter\"\n  return(function(data, formula){ \n    # Funkcja fitter tworzy sieć neuronową w ramach biblioteki nnet oraz tworzy definicję funkcji lines dla klasy data.frame. \n    # fitter\n    nn &lt;- nnet(formula, data, size = c(size), maxit=1000, rang = 0.1, decay = 5e-4, linout=T)\n    ## definicja f. lines dla klasy data.frame\n    e &lt;- parent.env(environment())\n    e$lines &lt;- function(x, ...) {\n      graphics::lines(x = x[,1], y = x[,2], ...)\n    }\n    \n    return(function(newData){\n      #Funkcja predyktor\n      #Zwraca dataframe ktory ma dwie kolumny, x ze zbioru wartości, których model wcześniej nie widział i y wyliczone na podstawie sieci neuronowej\n      out &lt;- data.frame(newData[,1], predict(nn, newData))\n      return(out)\n    })\n  })\n}\n\n\n\nPrzykłady\n\nPrzykład 1\n\n### Tworzenie przykładowych danych - zbiór treningowy\nd &lt;- tibble(\n  x = rnorm(10^3),\n  y = x^2 + 3 * cos(x) + rnorm(10^3, sd = 0.4)\n)\n\n### Tworzenie zbioru testowego\ndTest &lt;- tibble(\n  x = seq(\n    from = min(d$x),\n    to = max(d$x),\n    length.out = 100)) ## features\n\n\n### Tworzenie fittera. Poniżej wykorzystujemy jedynie 10 neuronów w\n### warstwie ukrytej.\nfitter &lt;- createFitter(size = 10)\n\n### Tworzenie predyktora. \npredictor &lt;- fitter(formula = y ~ x, data = d)\n\n\n### Wizualizacja przykładowych danych wyniku\nplot(d, pch = 20,\n     col = rgb(1, 0, .5, 0.2),\n     xlab = \"feature\", ylab = \"target\")\ngrid(lty = \"solid\", col = \"lightgray\")\n\nlines(x = predictor(newData = dTest),\n      col = rgb(.5, 0, 1, .9),\n      lwd = 2)\n\n\n\n\nPrzykład 1\n\n\n\n\n\n\nPrzykład 2\n\n### Tworzenie przykładowych danych - zbiór treningowy\nd &lt;- tibble(\n  x = rnorm(10^3),\n  y = x^2 + 3 * cos(x) + rnorm(10^3, sd = 0.4)\n)\n\n### Tworzenie zbioru testowego\ndTest &lt;- tibble(\n  x = seq(\n    from = min(d$x),\n    to = max(d$x),\n    length.out = 100)) ## features\n\n\n### Tworzesnie fittera. Poniżej wykorzystujemy jedynie 10 neuronów w\n### warstwie ukrytej.\nfitter &lt;- createFitter(size = 3)\n\n### Tworzenie predyktora. \npredictor &lt;- fitter(formula = y ~ x, data = d)\n\n\n### Wizualizacja przykładowych danych wyniku\n\nplot(d, pch = 20,\n     col = rgb(1, 0, .5, 0.2),\n     xlab = \"feature\", ylab = \"target\")\ngrid(lty = \"solid\", col = \"lightgray\")\n\nlines(x = predictor(newData = dTest),\n      col = rgb(.5, 0, 1, .9),\n      lwd = 2)\n\n\n\n\nPrzykład 2\n\n\n\n\n\n\nPrzyklad 3\n\n### Tworzenie przykładowych danych - zbiór treningowy\nd1 &lt;- tibble(\n  x = rnorm(10^3, mean = -5),\n  y = (x + 5)^2 + 3 * cos(x) + rnorm(10^3, sd = 0.4)\n)\nd2 &lt;- tibble(\n  x = rnorm(10^3, mean = 5),\n  y = (x - 5)^2 + 3 * cos(x) + rnorm(10^3, sd = 0.4)\n)\nd &lt;- rbind(d1, d2)\n\n### Tworzenie zbioru testowego\ndTest &lt;- tibble(\n  x = seq(\n    from = min(d$x),\n    to = max(d$x),\n    length.out = 100)) ## features\n\n\n### Tworzesnie fittera. Poniżej wykorzystujemy jedynie 10 neuronów w\n### warstwie ukrytej.\nfitter3 &lt;- createFitter(size = 3)\nfitter20 &lt;- createFitter(size = 20)\n\n### Tworzenie predyktora. \npredictor3 &lt;- fitter3(formula = y ~ x, data = d)\npredictor20 &lt;- fitter20(formula = y ~ x, data = d)\n\n\n### Wizualizacja przykładowych danych wyniku\n#| label: przyklad-3\n#| fig-cap: \"Przykład 3\"\n#| warning: false\nplot(d, pch = 20,\n     col = rgb(1, 0, .5, 0.2),\n     xlab = \"feature\", ylab = \"target\")\ngrid(lty = \"solid\", col = \"lightgray\")\nlines(x = predictor3(newData = dTest),\n      col = rgb(.5, 0, 1, .9),\n      lwd = 2)\nlines(x = predictor20(newData = dTest),\n      col = rgb(0, .5, 1, .9),\n      lwd = 2)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  }
]